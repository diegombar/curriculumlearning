{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks andÂ Beyond\n",
    "\n",
    "In this iPython notebook I implement a Deep Q-Network using both Double DQN and Dueling DQN. The agent learn to solve a navigation task in a basic grid world. To learn more, read here: https://medium.com/p/8438a3e2b8df\n",
    "\n",
    "For more reinforcment learning tutorials, see:\n",
    "https://github.com/awjuliani/DeepRL-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "# import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Feel free to adjust the size of the gridworld. Making it smaller provides an easier task for our DQN agent, while making the world larger increases the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82a0610160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False,size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Starting state of the grid world. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to the green square (+1 reward) and avoid the red square (-1 reward). There is a -0.01 penalty at each step. The starting positions of the blocks are always the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Implementing the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network receives a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(xavier_init([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This class allows us to store experies and sample then randomly to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is a simple function to resize our game frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These functions allow us to update the parameters of our target network with those of the primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setting all the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(total_steps,np.mean(rList[-10:]), e)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Checking network learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
