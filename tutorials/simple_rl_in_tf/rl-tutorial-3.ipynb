{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL\n",
    "In this iPython notebook we implement a policy and model network which work in tandem to solve the CartPole reinforcement learning problem. To learn more, read here: https://medium.com/p/9a6fe0cce99\n",
    "\n",
    "For more reinforcment learning tutorials, see:\n",
    "https://github.com/awjuliani/DeepRL-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# from modelAny import *\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import rnn\n",
    "# from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries and starting CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-25 17:14:34,059] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 8 # number of hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "\n",
    "model_bs = 3 # Batch size when learning from model\n",
    "real_bs = 3 # Batch size when learning from real environment\n",
    "\n",
    "# model initialization\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None,4] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network\n",
    "Here we implement a multi-layer neural network that predicts the next observation, reward, and done state from a current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5])\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None,5] , name=\"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]),name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state,W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]),name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M,W2M) + B2M)\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32,[None,4],name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32,[None,1],name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32,[None,1],name=\"true_done\")\n",
    "\n",
    "\n",
    "predicted_state = tf.concat([predicted_observation,predicted_reward,predicted_done],1)\n",
    "\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "\n",
    "done_loss = tf.multiply(predicted_done, true_done) + tf.multiply(1-predicted_done, 1-true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "        \n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous state and action\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Policy and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(1, 4), b.shape=(4, 8), m=1, n=8, k=4\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_input_x_0/_5, W1/read)]]\n\t [[Node: Sigmoid/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'MatMul', defined at:\n  File \"/home/diego/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/diego/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-dd79e93e7d77>\", line 5, in <module>\n    layer1 = tf.nn.relu(tf.matmul(observations,W1))\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1801, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1263, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(1, 4), b.shape=(4, 8), m=1, n=8, k=4\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_input_x_0/_5, W1/read)]]\n\t [[Node: Sigmoid/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(1, 4), b.shape=(4, 8), m=1, n=8, k=4\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_input_x_0/_5, W1/read)]]\n\t [[Node: Sigmoid/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ac0cc7865345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtfprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtfprob\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(1, 4), b.shape=(4, 8), m=1, n=8, k=4\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_input_x_0/_5, W1/read)]]\n\t [[Node: Sigmoid/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'MatMul', defined at:\n  File \"/home/diego/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/diego/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-dd79e93e7d77>\", line 5, in <module>\n    layer1 = tf.nn.relu(tf.matmul(observations,W1))\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1801, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1263, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/diego/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(1, 4), b.shape=(4, 8), m=1, n=8, k=4\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_input_x_0/_5, W1/read)]]\n\t [[Node: Sigmoid/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation,[1,4])\n",
    "\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) \n",
    "        y = 1 if action == 0 else 0 \n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess,xs,action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1,:]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:,:]\n",
    "                rewards = np.array(epr[1:,:])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts,rewards,dones])\n",
    "\n",
    "                feed_dict={previous_state: state_prevs, true_observation: state_nexts,true_done:dones,true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss,predicted_state,updateModel],feed_dict)\n",
    "            if trainThePolicy == True:\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number: \n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                if drawFromModel == False:\n",
    "                    print('World Perf: Episode %f. Reward %f. action: %f. mean reward %f.' % (real_episodes,reward_sum/real_bs,action, running_reward/real_bs))\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes, we start alternating between training the policy\n",
    "                # from the model and training the model from the real environment.\n",
    "                if episode_number > 100:\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print(real_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model representation\n",
    "Here we can examine how well the model is able to approximate the true environment after training. The green line indicates the real environment, and the blue indicates model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(6):\n",
    "    plt.subplot(6, 2, 2*i + 1)\n",
    "    plt.plot(pState[:,i])\n",
    "    plt.subplot(6,2,2*i+1)\n",
    "    plt.plot(state_nextsAll[:,i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
